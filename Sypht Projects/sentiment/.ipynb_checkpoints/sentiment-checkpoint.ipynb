{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88051c3d-4a44-4ec6-8adb-c1d38653e94d",
    "_uuid": "71fff663eb9075c63db6ddd0904379ffbcdcad20"
   },
   "source": [
    "# Machine Learning Challenge\n",
    "\n",
    "## Overview\n",
    "\n",
    "The focus of this exercise is on a field within machine learning called [Natural Language Processing](https://en.wikipedia.org/wiki/Natural-language_processing). We can think of this field as the intersection between language, and machine learning. Tasks in this field include automatic translation (Google translate), intelligent personal assistants (Siri), information extraction, and speech recognition for example.\n",
    "\n",
    "NLP uses many of the same techniques as traditional data science, but also features a number of specialised skills and approaches. There is no expectation that you have any experience with NLP, however, to complete the challenge it will be useful to have the following skills:\n",
    "\n",
    "- understanding of the python programming language\n",
    "- understanding of basic machine learning concepts, i.e. supervised learning\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Download this notebook!\n",
    "2. Answer each of the provided questions, including your source code as cells in this notebook.\n",
    "3. Share the results with us, e.g. a Github repo.\n",
    "\n",
    "### Task description\n",
    "\n",
    "You will be performing a task known as [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis). Here, the goal is to predict sentiment -- the emotional intent behind a statement -- from text. For example, the sentence: \"*This movie was terrible!\"* has a negative sentiment, whereas \"*loved this cinematic masterpiece*\" has a positive sentiment.\n",
    "\n",
    "To simplify the task, we consider sentiment binary: labels of `1` indicate a sentence has a positive sentiment, and labels of `0` indicate that the sentence has a negative sentiment.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset is split across three files, representing three different sources -- Amazon, Yelp and IMDB. Your task is to build a sentiment analysis model using both the Yelp and IMDB data as your training-set, and test the performance of your model on the Amazon data.\n",
    "\n",
    "Each file can be found in the `input` directory, and contains 1000 rows of data. Each row contains a sentence, a `tab` character and then a label -- `0` or `1`. \n",
    "\n",
    "**Notes**\n",
    "- Feel free to use existing machine learning libraries as components in you solution!\n",
    "- Suggested libraries: `sklearn` (for machine learning), `pandas` (for loading/processing data), `spacy` (for text processing).\n",
    "- As mentioned, you are not expected to have previous experience with this exact task. You are free to refer to external tutorials/resources to assist you. However, you will be asked to justfify the choices you have made -- so make you understand the approach you have taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazon_cells_labelled.txt', 'imdb_labelled.txt', 'yelp_labelled.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"./input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So there is no way for me to plug it in here in the US unless I go by a converter.\t0\n",
      "Good case, Excellent value.\t1\n",
      "Great for the jawbone.\t1\n",
      "Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\t0\n",
      "The mic is great.\t1\n",
      "I have to jiggle the plug to get it to line up right to get decent volume.\t0\n",
      "If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.\t0\n",
      "If you are Razr owner...you must have this!\t1\n",
      "Needless to say, I wasted my money.\t0\n",
      "What a waste of money and time!.\t0\n"
     ]
    }
   ],
   "source": [
    "!head \"./input/amazon_cells_labelled.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "387106cd-e89a-462f-b204-a91a73d12137",
    "_uuid": "cbd1a4b1d16ce7db6def7b3b393b48618d7e4777"
   },
   "source": [
    "# Tasks\n",
    "### 1. Read and concatenate data into test and train sets.\n",
    "### 2. Prepare the data for input into your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a8240a39-7002-435b-ba45-ac859d209f7f",
    "_uuid": "69c6d7ea240a191abfaaf00574f09521944387d7"
   },
   "source": [
    "#### 2a: Find the ten most frequent words in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f4cc399e-66c4-4bf7-a8e1-03711372c7b4",
    "_uuid": "eb8b033dc4a841702ae52d4ec71e7718b3257dda"
   },
   "source": [
    "### 3. Train your model and justify your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1a5840b2-c84c-42f6-9fc9-4fed64e48298",
    "_uuid": "f4eeecd64b54cc05098affe6cca4c40204af8ecf"
   },
   "source": [
    "### 4. Evaluate your model using metric(s) you see fit and justify your choices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
